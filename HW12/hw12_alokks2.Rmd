---
title: 'STAT 420: Homework 12'
author: "Alok K. Shukla (alokks2)"
date: "12/04/2016"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
  pdf_document:
    fig_caption: yes
    highlight: tango
    toc: yes
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
```


# Assignment Solutions

## Exercise 1 (Does It Work?)

In this exercise, we will investigate how well backwards AIC and BIC actually perform. For either to be "working" correctly, when used, they should result in a low number of both **false positives** and **false negatives**. In model selection,

- **False Positive**, FP: Incorrectly including a variable in the model. Including a *non-significant* variable.
- **False Negative**, FN: Incorrectly excluding a variable in the model. Excluding a *significant* variable.

Consider the **true** model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_7 x_7 + \beta_8 x_8 + \beta_9 x_9 + \beta_{10} x_{10} + \epsilon
\]

where $\epsilon \sim N(0, \sigma^2 = 4)$. The true values of the $\beta$ parameters are given in the `R` code below.

```{r}
beta_0  = 1
beta_1  = 1
beta_2  = 1
beta_3  = 1
beta_4  = 1
beta_5  = 0
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
```

Then, as we have specified them, some variables are significant, and some are not. We store their names in `R` variables for use later.

```{r}
signif  = c("(Intercept)", "x_1", "x_2", "x_3", "x_4")
not_sig = c("x_5", "x_6", "x_7", "x_8", "x_9", "x_10")
```

We now simulate values for these `x` variables, which we will use throughout part **(a)**.

```{r}
set.seed(42)
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
```

We then combine these into a data frame, and simulate `y` according to the true model.

```{r}
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + rnorm(n, 0 , sigma)
)
```

We do a quick check to make sure everything looks correct.

```{r}
head(sim_data_1)
```

Now, we fit an incorrect model.

```{r}
fit = lm(y ~ x_4 + x_5 + x_6, data = sim_data_1)
coef(fit)
```

Notice, we have coefficients for `x_4`, `x_5`, and `x_6`. This means that `x_5` and `x_6` are false positives, while `x_1`, `x_2`, and `x_3` are false negatives.

To detect the false negatives, use:

```{r}
# which are false negatives?
!(signif %in% names(coef(fit)))
```

To detect the false positives, use:

```{r}
# which are false positives?
names(coef(fit)) %in% not_sig
```

Note that in both cases, you could `sum()` the result to obtain the number of false negatives or positives.

**(a)** Set a seed equal to your birthday; then, using the given data for each `x` variable above in `sim_data_1`, simulate the response variable `y` 200 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods.

**Solution**

```{r}
set.seed(19920120)
num_sim = 200
fn_aic = rep(0,num_sim)
fp_aic = rep(0,num_sim)
fn_bic = rep(0,num_sim)
fp_bic = rep(0,num_sim)
fn_aic_rate = rep(0,num_sim)
fp_aic_rate = rep(0,num_sim)
fn_bic_rate = rep(0,num_sim)
fp_bic_rate = rep(0,num_sim)
for(i in 1:num_sim){
  #Simulate the response variable `y`
  sim_data_1$y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + rnorm(n, 0 , sigma)
  #Fit an additive model using each of the `x` variables.
  model = lm(y~.,data=sim_data_1)
  #Perform variable selection using backwards AIC.
  model_back_aic = step(model, direction = "backward",trace = 0)
  #Perform variable selection using backwards BIC.
  model_back_bic = step(model, direction = "backward", k = log(length(resid(model))),trace = 0)
  #Calculate and store the number of false negatives for the models chosen by AIC and BIC.
  fn_aic[i] = sum(!(signif %in% names(coef(model_back_aic))))
  fn_bic[i] = sum(!(signif %in% names(coef(model_back_bic))))
  #Calculate and store the number of false positives for the models chosen by AIC and BIC.
  fp_aic[i] = sum(names(coef(model_back_aic)) %in% not_sig)
  fp_bic[i] = sum(names(coef(model_back_bic)) %in% not_sig)
  #Calculate the rate of false positives and negatives for both AIC and BIC.
  fp_aic_rate[i] = fp_aic[i]/(fp_aic[i]+(sum(!(not_sig %in% names(coef(model_back_aic))))))
  fp_bic_rate[i] = fp_bic[i]/(fp_bic[i]+(sum(!(not_sig %in% names(coef(model_back_bic))))))
  fn_aic_rate[i] = fn_aic[i]/(fn_aic[i]+(sum(signif %in% names(coef(model_back_aic)))))
  fn_bic_rate[i] = fn_bic[i]/(fn_bic[i]+(sum(signif %in% names(coef(model_back_bic)))))
}

#Compare the rates between the two methods.
mean(fp_aic_rate)
mean(fp_bic_rate)
mean(fn_aic_rate)
mean(fn_bic_rate)
par(mfrow=c(1,2))
plot(fp_aic_rate,type="l",col="dodgerblue",main="Orange: BIC, Blue: AIC",xlab="Model 1 Simulation#",ylab="FPR")
lines(fp_bic_rate,col="orange")
plot(fn_aic_rate,type="b",col="dodgerblue",main="Orange: BIC, Blue: AIC",xlab="Model 1 Simulation#",ylab="FNR")
lines(fn_bic_rate,col="orange")

```


**(b)** Set a seed equal to your birthday; then, using the given data for each `x` variable below in `sim_data_2`, simulate the response variable `y` 200 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Also compare to your answers in part **(a)** and give a possible reason for any difference.

```{r}
set.seed(42)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)

sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + rnorm(n, 0 , sigma)
)
```

**Solution**
```{r}
set.seed(19920120)
num_sim = 200
fn_aic = rep(0,num_sim)
fp_aic = rep(0,num_sim)
fn_bic = rep(0,num_sim)
fp_bic = rep(0,num_sim)
fn_aic_rate = rep(0,num_sim)
fp_aic_rate = rep(0,num_sim)
fn_bic_rate = rep(0,num_sim)
fp_bic_rate = rep(0,num_sim)
for(i in 1:num_sim){
  #Simulate the response variable `y`
  sim_data_2$y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + rnorm(n, 0 , sigma)
  #Fit an additive model using each of the `x` variables.
  model = lm(y~.,data=sim_data_2)
  #Perform variable selection using backwards AIC.
  model_back_aic = step(model, direction = "backward",trace = 0)
  #Perform variable selection using backwards BIC.
  model_back_bic = step(model, direction = "backward", k = log(length(resid(model))),trace = 0)
  #Calculate and store the number of false negatives for the models chosen by AIC and BIC.
  fn_aic[i] = sum(!(signif %in% names(coef(model_back_aic))))
  fn_bic[i] = sum(!(signif %in% names(coef(model_back_bic))))
  #Calculate and store the number of false positives for the models chosen by AIC and BIC.
  fp_aic[i] = sum(names(coef(model_back_aic)) %in% not_sig)
  fp_bic[i] = sum(names(coef(model_back_bic)) %in% not_sig)
  #Calculate the rate of false positives and negatives for both AIC and BIC.
  fp_aic_rate[i] = fp_aic[i]/(fp_aic[i]+(sum(!(not_sig %in% names(coef(model_back_aic))))))
  fp_bic_rate[i] = fp_bic[i]/(fp_bic[i]+(sum(!(not_sig %in% names(coef(model_back_bic))))))
  fn_aic_rate[i] = fn_aic[i]/(fn_aic[i]+(sum(signif %in% names(coef(model_back_aic)))))
  fn_bic_rate[i] = fn_bic[i]/(fn_bic[i]+(sum(signif %in% names(coef(model_back_bic)))))
}

#Compare the rates between the two methods.
mean(fp_aic_rate)
mean(fp_bic_rate)
mean(fn_aic_rate)
mean(fn_bic_rate)
par(mfrow=c(1,2))
plot(fp_aic_rate,type="l",col="dodgerblue",main="Orange: BIC, Blue: AIC",xlab="Model 2 Simulation#",ylab="FPR")
lines(fp_bic_rate,col="orange")
plot(fn_aic_rate,type="l",col="dodgerblue",main="Orange: BIC, Blue: AIC",xlab="Model 2 Simulation#",ylab="FNR")
lines(fn_bic_rate,col="orange")


```
<br/>
*Compare to your answers in part **(a)** and give a possible reason for any difference.*

1. FPR Comparison

  Looking at the plots we can say that FPR has increased for both AIC and BIC.

2. FNR Comparison

  The FNR for BIC has increased more than FNR for AIC.

3. Possible reason

  Multicollinearity


## Exercise 2 (Body Dimensions)

For this exercise we will use the data stored in [`body.csv`](body.csv). It contains 21 body dimension measurements as well as age, weight, height, and gender on 507 individuals. The participants were primarily individuals in their twenties and thirties, with a few older men and women, all of whom proclaimed to exercise several hours a week. The variables in the dataset are:

Skeletal Measurements (all measured in cm):

- `s1` - Biacromial diameter
- `s2` - Biiliac diameter, or "pelvic breadth"
- `s3` - Bitrochanteric diameter
- `s4` - Chest depth between spine and sternum at nipple level, mid-expiration
- `s5` - Chest diameter at nipple level, mid-expiration
- `s6` - Elbow diameter, sum of two elbows
- `s7` - Wrist diameter, sum of two wrists
- `s8` - Knee diameter, sum of two knees
- `s9` - Ankle diameter, sum of two ankles

Girth Measurements (all measured in cm):

- `g1` - Shoulder girth over deltoid muscles
- `g2` - Chest girth, nipple line in males and just above breast tissue in females, mid-expiration
- `g3` - Waist girth, narrowest part of torso below the rib cage, average of contracted and relaxed position
- `g4` - Navel (or "Abdominal") girth at umbilicus and iliac crest, iliac crest as a landmark
- `g5` - Hip girth at level of bitrochanteric diameter
- `g6` - Thigh girth below gluteal fold, average of right and left girths
- `g7` - Bicep girth, flexed, average of right and left girths
- `g8` - Forearm girth, extended, palm up, average of right and left girths
- `g9` - Knee girth over patella, slightly flexed position, average of right and left girths
- `g10` - Calf maximum girth, average of right and left girths
- `g11` - Ankle minimum girth, average of right and left girths
- `g12` - Wrist minimum girth, average of right and left girths

Other Measurements:

- `Age` - in years
- `Weight` - in kg
- `Height` - in cm
- `Gender` - 0 = female, 1 = male
```{r,warning=FALSE}
body <- read.csv("body.csv")
library(leaps)
calc_loocv_rmse = function(model) {
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```


**(a)** Find a good model for `Weight` using only the skeletal variables as well as `Age`, `Height`, and `Gender`. You are not allowed to remove any data or transform the response.

**Solution**
```{r}
weight_model = summary(regsubsets(Weight~Age+Height+Gender+s1+s2+s3+s4+s5+s6+s7+s8+s9,data=body))
(best_r2_ind = which.max(weight_model$adjr2))
weight_model$which[best_r2_ind, ]
# Lets fit this model
(weight_best_r2_model_s = lm(Weight~Age+Height+s2+s3+s4+s5+s7+s8,data = body))
calc_loocv_rmse(weight_best_r2_model_s)
```




**(b)** Find a good model for `Weight` using only the girth variables as well as `Age`, `Height`, and `Gender`. You are not allowed to remove any data or transform the response.

**Solution**
```{r}
weight_model = summary(regsubsets(Weight~Age+Height+Gender+g1+g2+g3+g4+g5+g6+g7+g8+g9+g10+g11+g12,data=body))
(best_r2_ind = which.max(weight_model$adjr2))
weight_model$which[best_r2_ind, ]
# Lets fit this model
(weight_best_r2_model_g = lm(Weight~Height+g2+g3+g5+g6+g8+g9+g10,data = body))
calc_loocv_rmse(weight_best_r2_model_g)
```




**(c)** Based on **(a)** and **(b)**, which set of body measurements are more useful for predicting `Weight`?

**Solution**
```{r}
summary(weight_best_r2_model_g)$adj.r.squared >summary(weight_best_r2_model_s)$adj.r.squared
calc_loocv_rmse(weight_best_r2_model_g)<calc_loocv_rmse(weight_best_r2_model_s)
```

That suggests Girth Measurements are better predictors of `Weight`.

**(d)** Using all available variables, devise and justify a good model for predicting `Weight`. It should use less than 50 parameters and obtain a LOOCV RMSE less than 1.9. You are not allowed to remove any data or transform the response.

**Solution**
```{r}
# Lets see how many parameters we should be estimating in one model
nrow(body)/3

# All 2 - way interactions
two_int = lm(Weight~.^2,data = body)
# Too many
length(coef(two_int))
# Create three models to accomodate all interactions (randomly sampled)
model1=lm(Weight~.+g4:Gender+s8:Age+s2:g6+s9:Age+s1:g11+s6:s7+g1:Age+g3:g11+s2:s9+s1:s8+g3:Age+
g2:g7+s7:g10+s3:Height+g9:Gender+g8:g12+s5:g5+s7:g11+s5:Gender+g5:g9+s1:s9+s5:g6+
s7:Height+s8:g3+s5:s9+s2:g10+s6:g6+s5:g12+g11:g12+s3:s7+g6:g12+g7:g11+s3:g5+
s1:Height+s3:s8+s9:g10+s5:s7+s2:s3+s6:g7+g4:g5+s2:g3+s2:s8+s1:Age+g9:Height+
s9:Height+s8:g10+g4:g9+s8:g12+s6:g9+g1:g10+g8:g9+s8:s9+s1:g9+s8:g5+s6:g11+
g7:Height+g1:g5+g1:g3+g10:g12+s8:g9+s1:g8+g3:g9+g7:g10+s5:g1+s3:g2+g2:g11+
s1:g5+g7:g9+g1:g2+s9:g5+s5:s6+s1:g2+g1:g4+g5:g12+s5:g4+s9:g4+g1:Gender+
g12:Gender+s8:Height+g1:g11+s3:s9+s4:Age+s4:s8+g7:Age+g2:Height+g7:g12+s4:g12+Age:Gender+
g1:g6+s7:g4+g4:Age+s4:g7+s9:Gender+s2:g8+s7:g3+s3:g4+s3:s4+s8:g6+s3:g12,data=body)

model2=lm(Weight~.+s2:Height+s9:g3+s3:s6+s3:Gender+s1:s7+s1:g3+g8:g11+g6:g10+g5:g7+Age:Height+s5:g10+
g4:g12+s6:g2+s1:s4+s1:g4+s1:g6+g3:g7+s4:g8+g6:g11+s2:Age+s6:s8+g9:g10+
s6:g5+g5:g6+s8:g8+s6:Gender+s2:g12+s8:Gender+g3:g12+s9:g9+s7:g1+s4:s7+s3:g10+
s8:g7+g12:Age+s4:Height+s7:g9+s1:s2+s6:Age+s5:g9+g12:Height+g3:g5+s3:Age+g7:g8+
g3:g8+s7:g8+s1:Gender+g1:g12+s2:s5+s4:g11+s3:g7+g4:g7+s1:s5+s3:g1+s4:Gender+
s2:s7+s5:g3+g5:g8+g10:g11+s7:Gender+g2:g5+g9:g11+g6:Age+g2:g8+s3:g6+g5:Age+
g5:Gender+s3:g8+s4:g5+s4:g1+s2:Gender+s2:g5+s2:s6+s6:g4+g1:g7+s2:g4+s6:g12+
s5:Age+s7:g5+g3:g10+g6:Gender+s9:g12+g3:Gender+s5:g7+g4:g6+s7:g12+s5:g8+s7:g2,data=body)

model3 = lm(Weight~.+g10:Height+s4:g2+s2:g11+s7:Age+g6:g8+g2:g4+s3:g11+s5:Height+s9:g11+s1:s6+s4:s6+
s9:g1+s4:g6+s7:s9+g2:g10+s5:s8+s7:g7+s7:g6+g4:g11+g5:g11+s6:g8+s9:g6+
s4:g4+s1:g7+g11:Gender+g3:g6+g9:g12+Height:Gender+g10:Age+g1:g9+g6:Height+g4:g10+g3:g4+
s1:g1+g1:Height+s6:g1+s1:g12+g8:Age+g2:g9+s1:s3+s2:g1+g4:Height+g3:Height+s8:g11+
g5:Height+s1:g10+g7:Gender+g2:g12+g6:g9+s4:s9+g1:g8+g2:Gender+s3:s5+s3:g9+s4:s5+
g2:g6+s6:Height+s7:s8+g10:Gender+g9:Age+g6:g7+s6:g10+g4:g8+s8:g2+s9:g7+s8:g4+
g5:g10+s6:g3+g11:Age+s2:g9+s5:g11+g8:g10+s8:g1+s4:g3+s4:g9+g2:Age+s3:g3+
s4:g10+g8:Height+g8:Gender+s6:s9+s2:g7+s9:g8+g11:Height+s5:g2+s9:g2+g2:g3+s2:s4+
s2:g2,data=body)

# Choose significant from these

model1_aic = step(model1,direction="backward",trace=0)
model2_aic = step(model2,direction="backward",trace=0)
model3_aic = step(model3,direction="backward",trace=0)

# How many signif parameters in total
length(coef(model3_aic))+length(coef(model2_aic))+length(coef(model1_aic))

# Combine these and quadratic terms
combined_model = lm(Weight~s1+s2+s3+s4+s5+s7+s8+s9+g1+g2+g3+g4+g5+g6+g7+g8+g9+g10+g11+g12+Age+Height+Gender+
s8:Age+s1:g11+g1:Age+g3:g11+s2:s9+s1:s8+g3:Age+g2:g7+s7:g10+g9:Gender+s7:g11+s5:g6+
g11:g12+s3:s7+g6:g12+g7:g11+s3:g5+s1:Height+s2:s3+s2:s8+s1:Age+s9:Height+s8:g10+s8:s9+
g1:g2+s9:g5+s1:g2+g1:g4+g5:g12+s5:g4+s9:g4+g12:Gender+s8:Height+Age:Gender+g1:g6+g4:Age+
s4:g7+s8:g6+s3:g12+s3:s6+s3:Gender+g8:g11+g5:g7+Age:Height+s5:g10+s6:g2+s1:s4+s1:g4+g3:g7+g6:g11+s6:g5+
s6:Gender+s2:g12+s7:g1+s3:g10+g12:Age+s4:Height+g3:g8+s2:s5+g4:g7+s1:s5+s5:g3+s7:Gender+
g2:g5+g6:Age+s4:g1+s2:Gender+s2:g4+s7:g5+s9:g12+g3:Gender+g4:g6+s5:g8+s7:g2+s4:g2+g6:g8+
g2:g4+s9:g11+s4:s6+s9:g1+g4:g11+g5:g11+s9:g6+g11:Gender+Height:Gender+g10:Age+g4:g10+
g8:Age+g6:g9+g1:g8+g2:Gender+g2:g6+s7:s8+s6:g10+s8:g4+g8:g10+s8:g1+g2:Age+g8:Gender+s9:g8+g2:g3+
I(s1^2)+I(s2^2)+I(s3^2)+I(s4^2)+I(s5^2)+I(s6^2)+I(s7^2)+I(s8^2)+I(s9^2)+I(g1^2)+I(g2^2)+I(g3^2)+
  I(g4^2)+I(g5^2)+I(g6^2)+I(g7^2)+I(g8^2)+I(g9^2)+I(g10^2)+I(g11^2)+I(g12^2)+I(Age^2)+I(Weight^2)+I(Height^2)+I(Gender^2),data = body)

# Number of parameters
length(coef(combined_model))

# LOOCV RMSE
calc_loocv_rmse(combined_model)

# Lets optimize
model_back_bic = step(combined_model, direction = "backward", k = log(length(resid(combined_model))),trace = 0)

# LOOCV RMSE
calc_loocv_rmse(model_back_bic)

# No. of parameters
length(coef(model_back_bic))
```
We have `43` parameters in our final model that gives LOOCV RMSE less than `1.9` .

## Exercise 3 (Ball Bearings)

For this exercise we will use the data stored in [`ballbearings.csv`](ballbearings.csv). It contains 210 observations, each of which reports the results of a test on a set of ball bearings. Manufacturers who use bearings in their products have an interest in their reliability. The basic measure of reliability in this context is the rating life, also known in engineering as fatigue failure. The objective is to model `L50`, the median lifetime of this sample of ball bearings. The variables in the dataset are:

- `L50` - median life: the number of revolutions that 50% of a group of identical bearings would be expected to achieve
- `P` - the load on the bearing in operation
- `Z` - the number of balls in the bearing
- `D` - the diameter of the balls
- `Company` - denotes who manufactured the ball bearing (A, B, C)
- `Type` - Company B makes several types of ball bearings (1, 2, 3); 0 otherwise

```{r}
bearings = read.csv("ballbearings.csv")
```


**(a)** Find a model for `log(L50)` that does not reject the Shapiro-Wilk test at $\alpha = 0.01$ and obtains an **adjusted** $R^2$ higher than 0.52. You may not remove any observations, but may consider transformations. Your model should use fewer than 10 $\beta$ parameters.

**Solution**
```{r}
# Default
L50_model = lm(log(L50)~.,data=bearings)
summary(L50_model)$adj.r.squared 
shapiro.test(resid(L50_model))

# Exhaustive Search
L50_model_all = summary(regsubsets(log(L50)~.,data=bearings))
(best_r2_ind = which.max(L50_model_all$adjr2))
# Better model
coef(L50_model)[L50_model_all$which[best_r2_ind,]]
L50_better_model = lm(log(L50)~ P+Z+D+Company,data=bearings)
summary(L50_better_model)$adj.r.squared 
shapiro.test(resid(L50_better_model))
# Interactions
L50_int = lm(log(L50)~(.)^2+I(P^2)+I(Z^2)+I(D^2),data=bearings)
summary(L50_int)$adj.r.squared 
shapiro.test(resid(L50_int))


# Even more interactions
L50_int_3 = lm(log(L50)~(.)^3+I(P^2)+I(Z^2)+I(D^2),data=bearings)
summary(L50_int_3)$adj.r.squared 
shapiro.test(resid(L50_int_3))
# Not okay
length(coef(L50_int_3))

# Optimize
L50_int_all = summary(regsubsets(log(L50)~(.)^3+I(P^2)+I(Z^2)+I(D^2),data=bearings))
(best_r2_ind = which.max(L50_int_all$adjr2))
coef(L50_int_3)[L50_int_all$which[best_r2_ind,]]
better_mod = lm(log(L50)~P+D+I(P^2)+P:D+D:Company+P:Z:Type+ P:D:Company+P:Company:Type+Z:D:Company,data=bearings)

# Evaluate
length(coef(better_mod))
summary(better_mod)$adj.r.squared 
shapiro.test(resid(better_mod))

# Optimize
model_back_bic = step(better_mod, direction = "backward", k = log(length(resid(better_mod))),trace = 0)
# Even more
model_final = lm(log(L50)~P+D+I(P^2)+P:D+P:Z:Type+D:Company:Z+P:Company:Type,data=bearings)

# Evaluate
summary(model_final)$adj.r.squared
length(coef(model_final))
shapiro.test(resid(model_final))

# Optimze

model = lm(log(L50)~P+P:D + D+I(P^2)  +P:Z:Type+P:Type:Company+D:Z,data=bearings)
# Evaluate
length(coef(model))
summary(model)$adj.r.squared
shapiro.test(resid(model))
```


**(b)** Find a model for `log(L50)` that does not reject the Shapiro-Wilk test at $\alpha = 0.01$ and obtains an **adjusted** $R^2$ higher than 0.60. You may not remove any observations, but may consider transformations. Your model should use fewer than 20 $\beta$ parameters.

**Solution**



```{r}
better_mod = lm(log(L50)~P+D+I(P^2)+P:D+D:Company+P:Z:Type+ P:D:Company+P:Company:Type+Z:D:Company,data=bearings)

# Evaluate
length(coef(better_mod))
summary(better_mod)$adj.r.squared 
shapiro.test(resid(better_mod))
```


## Exercise 4 (Free Points)

This exercise is worth 10 points. If you turn in this assignment, you get 10 points! Congratulations, you've made it to the end of the homework for the semester!
