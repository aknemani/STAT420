---
title: 'STAT 420: Homework 10'
author: "Alok K. Shukla (alokks2)"
date: "11/13/2016"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
  pdf_document:
    fig_caption: yes
    highlight: tango
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment Solutions

## Exercise 1 (TV Is Healthy?)

For this exercise we will use the `tvdoctor` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?tvdoctor` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
library(lmtest)
```

**(a)** Fit a simple linear regression with `life` as the response and `tv` as the predictor. Plot a scatterplot and add the fitting line. Check the assumptions of this model.

**Solution**

```{r}
life_fit = lm(life~tv,data=tvdoctor)
plot(life~tv,data=tvdoctor,col="dodgerblue",pch = 20, cex = 1.5)
abline(life_fit,col="darkorange",lwd=2)
```
<br/>
Lets check assumptions

```{r}
bptest(life_fit)
shapiro.test(resid(life_fit))
```

We find non-constant variance in residuals.



**(b)** Fit higher order polynomial models of degree 3, 5, and 7. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.

**Solution**


```{r}
fit_3 = lm(life ~ tv + I(tv^2) + I(tv^3),data=tvdoctor)
fit_5 = lm(life ~ tv + I(tv^2) + I(tv^3) + I(tv^4) + I(tv^5),data=tvdoctor)
fit_7 = lm(life ~ tv + I(tv^2) + I(tv^3) + I(tv^4) + I(tv^5) + I(tv^6) + I(tv^7),data=tvdoctor)
par(mfrow=c(1,3))
plot(fitted(fit_3), resid(fit_3), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals",main="3 Degree")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
plot(fitted(fit_5), resid(fit_5), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals",main="5 Degree")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
plot(fitted(fit_7), resid(fit_7), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals",main="7 Degree")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
```

Based on the plots; `fit_3` violates the constant variance assumption while the other two dont.

- Model Selection

```{r}
anova(fit_5,fit_7)
```

We see a high p-val, so we will stick with `fit_5`.

- Normality assumption

```{r}
shapiro.test(resid(fit_5))
```

We see a large p-val, thus we can not reject the null-hypothesis of residuals follwoing a normal distribution.

- Influential points

```{r}
tvdoctor[cooks.distance(fit_5)> 4 / length(cooks.distance(fit_5)),]
```

## Exercise 2 (Brains)

The data set `mammals` from the `MASS` package contains the average body weight in kilograms $(x)$ and the average brain weight in grams $(y)$ for $62$ species of land mammals. Use `?mammals` to learn more.

```{r, message = FALSE, warning = FALSE}
library(MASS)
```

**(a)** What are the smallest and largest body weights in the dataset?

**Solution**
```{r}
mammals[which.max(mammals$body),1]
mammals[which.min(mammals$body),1]
```

**(b)** What are the smallest and largest brain weights in the dataset?

**Solution**
```{r}
mammals[which.max(mammals$brain),2]
mammals[which.min(mammals$brain),2]
```

**(c)** Plot average brain weight $(y)$ versus average body weight $(x)$.

**Solution**
```{r}
plot(mammals$body,mammals$brain, col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Average Body Weight", ylab = "Avaerage Brain Weight")
```


**(d)** Fit a linear model with `brain` as the response and `body` as the predictor. Test for significance of regression. Do you think this is an appropriate model?

**Solution**
```{r}
slr = lm(brain~body,data=mammals)
summary(slr)
```

We find that the regression is significant.
<br/>
Lets see if this is an appropriate model.

```{r}
plot(fitted(slr), resid(slr), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
```
<br/>

The constant variation assumption is violated, thus inappropriate.

- *the log model*

```{r}
log_model = lm(log(brain)~body,data=mammals)
plot(fitted(log_model), resid(log_model), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
```


<br/>

The variance in this model is same on both sides.

**(e)** Since the body weights do range over more than one order of magnitude and are strictly positive, we will use $\log(\text{body weight})$ as our *predictor*, with no further justification. Use the Box-Cox method to verify that $\log(\text{brain weight})$ is then a "recommended" transformation of the *response* variable. That is, verify that $\lambda = 0$ is among the "recommended" values of $\lambda$ when considering,

\[
g_\lambda(y) = \beta_0 + \beta_1 \log(\text{body weight})+\epsilon
\]

Please include the relevant plot in your results, using an appropriate zoom onto the relevant values.

**Solution**

```{r}
model = lm(brain~log(body),data=mammals)
boxcox(model,lambda = seq(-0.25, 0.25, by = 0.05),plotit = TRUE )
```

<br/>

Using the Box-Cox method, we see that `Î» = 0` is both in the interval, and extremely close to the maximum, which suggests a transformation of the form $\log(\text{body weight})$.

**(f)** Fit the model justified in part **(e)**. That is, fit a model with $\log(\text{brain weight})$ as the response and $\log(\text{body weight})$ as a predictor. Plot $\log(\text{brain weight})$ versus $\log(\text{body weight})$ and add the regression line to the plot. Does a linear relationship seem to be appropriate here?

**Solution**

```{r}
correct_model = lm(log(brain)~log(body),data=mammals)
plot(log(brain) ~ log(body), data = mammals, col = "dodgerblue", pch = 20, cex = 1.5 )
abline(correct_model, col = "darkorange", lwd = 2)
```
<br/>

The linear relationship does seem appropriate here.

**(g)** Use a Q-Q plot to check the normality of the errors for the model fit in part **(f)**.

**Solution**
```{r}
qqnorm(resid(correct_model), main = "Normal Q-Q Plot", col = "darkorange")
qqline(resid(correct_model), col = "dodgerblue", lwd = 2)
```

<br/>

Plot suggests that normality assumption is not violated.

**(h)** Use the model from part **(f)** to predict the brain weight of a male Pikachu which, has a body weight of 13.4 pounds. (Pikachu would be mammals, right?) Construct a 99% prediction interval.

**Solution**
```{r}
pikachu  = data.frame(body=13.4)
predict(correct_model, pikachu, interval="prediction") 
```


## Exercise 3 (EPA Emissions Data, Redux)

For this exercise we will again use the data stored in [`epa2015.csv`](epa2015.csv). It contains detailed descriptions of 4,411 vehicles manufactured in 2015 that were used for fuel economy testing [as performed by the Environment Protection Agency]( https://www3.epa.gov/otaq/tcldata.htm).

**(a)** Recall the model we had finished with last time:

```{r}
epa2015 = read.csv("epa2015.csv")
co2_int = lm(CO2 ~ horse * type, data = epa2015)
```

Which looked like this:

```{r}
plot(CO2 ~ horse, data = epa2015, col = type)

int_coef = summary(co2_int)$coef[,1]

int_both    = int_coef[1]
int_car     = int_coef[1] + int_coef[3]
int_truck   = int_coef[1] + int_coef[4]

slope_both  = int_coef[2]
slope_car   = int_coef[2] + int_coef[5]
slope_truck = int_coef[2] + int_coef[6]

abline(int_both, slope_both, lwd = 3, col = "black")
abline(int_car, slope_car, lwd = 3, col = "red")
abline(int_truck, slope_truck, lwd = 3, col = "green")
```

Create a fitted vs residuals plot for this model. Do you believe the constant variance assumption has been violated?

**Solution**

```{r}
plot(fitted(co2_int), resid(co2_int), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
```

<br/>
Looking at the plot, we can say that the constant variance assumption has been violated.

**(b)** Fit the same model as **(a)** but with a logged response. Create a fitted vs residuals plot for this model. Compare to the previous. Do you believe the constant variance assumption has been violated? Any other assumptions?

**Solution**

```{r}
par(mfrow=c(1,2))
co2_log = lm(log(CO2)~horse*type,data=epa2015)
plot(fitted(co2_int), resid(co2_int), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
plot(fitted(co2_log), resid(co2_log), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
```

Constat variance assumption is not violated for the new model excpet for some points. Lets check other assumptions

```{r}
shapiro.test(resid(co2_log))
```

A very small p-val suggests the normality assumption is violated.

**(c)** Fit a model that has all of the terms from the model in **(b)** as well as a quadratic term for `horse`. Use `log(CO2)` as the response. Create a fitted vs residuals plot for this model. Compare to the previous. Comment on model assumptions.

```{r}
par(mfrow=c(1,2))
co2_horse = lm(log(CO2)~horse*type+I(horse^4),data=epa2015)
plot(fitted(co2_log), resid(co2_log), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
plot(fitted(co2_horse), resid(co2_horse), col = "dodgerblue", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)

```

Compared to previous, this model is better at not violating the constant variance assumption. 

- Model assumptions
-- Constant variance

```{r}
bptest(co2_horse)
```

-- Normality
```{r}
shapiro.test(resid(co2_horse))
```


**(d)** Perform further analysis of the model fit in part **(c)**. Can you find any violations of assumptions?

## Exercise 4 (Bigger Is Better?)

Consider the true model,

\[
Y = 3 - 4 x + \epsilon,
\]

where $\epsilon \sim N(\mu = 0, \sigma = 9)$.

We can simulate observations from this model. We choose a sample size of 40.

```{r}
n = 40
set.seed(42)
x = runif(n, 0 , 10)
y = 3 - 4 * x + rnorm(n, 0 , 3)
```

Consider two models, one small, one big. The small fits a SLR model. The big fits a polynomial model of degree 10.

```{r}
fit_slr = lm(y ~ x)
fit_big = lm(y ~ poly(x, 10))
```

The big model has a smaller RMSE.

```{r}
mean(resid(fit_slr) ^ 2)
mean(resid(fit_big) ^ 2)
```

However, it is not significant when compared to the small.

```{r}
anova(fit_slr, fit_big)
```

By plotting the data and adding the two models, we see the the degree 10 polynomial is *very* wiggly. 

```{r}
plot(x, y, pch = 20, cex = 2)
abline(fit_slr, col = "darkorange", lwd = 3)
lines(seq(0, 10, 0.01), 
      predict(fit_big, newdata = data.frame(x = seq(0, 10, 0.01))), 
      col = 'dodgerblue', lwd = 3) 
```

**(a)** Use the following code after changing `birthday` to your birthday.

```{r}
num_sims = 1000
rmse_slr = rep(0, num_sims)
rmse_big = rep(0, num_sims)
pval     = rep(0, num_sims)
birthday = 19920120
set.seed(birthday)
```

Repeat the above process, keeping `x` the same, then re-generating `y` and fitting the SLR and big models `1000` times. Each time, store the RMSE of each model, and the p-value for comparing the two. (In the appropriate variables defined above.)

**Solution**

```{r}
for (i in 1:num_sims){
  y = 3 - 4 * x + rnorm(n, 0 , 3)
  fit_slr = lm(y ~ x)
  fit_big = lm(y ~ poly(x, 10))
  rmse_slr[i] = mean(resid(fit_slr) ^ 2)
  rmse_big[i] = mean(resid(fit_big) ^ 2)
  pval[i] = anova(fit_slr,fit_big)$"Pr(>F)"[2]
}
```


**(b)** What proportion of the RMSEs of the SLR model are smaller than the big model?

**Solution**

```{r}
mean(rmse_slr<rmse_big)
```
The `Big` model always has lower `RMSE`.

**(c)** What proportion of the p-values are less than 0.05?
```{r}
mean(pval<0.05)
```

Only a very small fraction of p-val is saying that the bigger model is significant.

**(d)** Do you think bigger is better?

**Solution**

Bigger models are overfitting; not better.

